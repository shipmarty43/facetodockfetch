# GPU Setup Guide

–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ CUDA/GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏—Ü.

üîó **Repository:** [https://github.com/shipmarty43/facetodockfetch](https://github.com/shipmarty43/facetodockfetch)

> **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –ü—Ä–æ–µ–∫—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Conda –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏. GPU –≤–µ—Ä—Å–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç `environment-gpu.yml` —Å CUDA-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π PyTorch, CUDA –∏ –¥—Ä—É–≥–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫.

## –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### GPU
- NVIDIA GPU —Å CUDA Compute Capability 3.5+ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 7.0+)
- –ú–∏–Ω–∏–º—É–º 4GB VRAM (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 8GB+)
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∫–∞—Ä—Ç—ã:
  - RTX 30xx/40xx —Å–µ—Ä–∏–∏ (–∏–¥–µ–∞–ª—å–Ω–æ)
  - GTX 16xx —Å–µ—Ä–∏–∏
  - RTX 20xx —Å–µ—Ä–∏–∏
  - Quadro —Å–µ—Ä–∏–∏

### –î—Ä–∞–π–≤–µ—Ä—ã
- NVIDIA Driver: 450.80.02 –∏–ª–∏ –Ω–æ–≤–µ–µ
- CUDA Toolkit: 11.8 (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤ Docker)

## –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ GPU
nvidia-smi

# –î–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU, –Ω–∞–ø—Ä–∏–º–µ—Ä:
# | NVIDIA GeForce RTX 3080    | 00000000:01:00.0 | 10GB |
```

## –í–∞—Ä–∏–∞–Ω—Ç 1: Docker Compose —Å GPU

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ NVIDIA Container Toolkit

```bash
# Ubuntu/Debian
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

### –ó–∞–ø—É—Å–∫ —Å GPU

```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –≤–µ—Ä—Å–∏—é docker-compose
docker-compose -f docker-compose.gpu.yml up -d

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ GPU –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
docker-compose -f docker-compose.gpu.yml exec backend python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

# –î–æ–ª–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏: CUDA available: True
```

### –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è

```bash
# –¢–æ –∂–µ —Å–∞–º–æ–µ, –Ω–æ —Å GPU compose —Ñ–∞–π–ª–æ–º
docker-compose -f docker-compose.gpu.yml exec backend python scripts/init_db.py
docker-compose -f docker-compose.gpu.yml exec backend python scripts/init_elasticsearch.py
docker-compose -f docker-compose.gpu.yml exec backend python scripts/create_admin.py --username admin --password admin123
```

## –í–∞—Ä–∏–∞–Ω—Ç 2: Conda —Å GPU

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ CUDA

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å CUDA toolkit —á–µ—Ä–µ–∑ conda
conda install -c conda-forge cudatoolkit=11.8

# –ò–ª–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π CUDA (Ubuntu)
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
sudo sh cuda_11.8.0_520.61.05_linux.run
```

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Python –ø–∞–∫–µ—Ç–æ–≤

```bash
# –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –æ–∫—Ä—É–∂–µ–Ω–∏–µ
conda activate face-recognition-system

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å PyTorch —Å CUDA
pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 \
  --extra-index-url https://download.pytorch.org/whl/cu118

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å ONNX Runtime GPU
pip install onnxruntime-gpu==1.16.3

# –ü—Ä–æ–≤–µ—Ä–∫–∞
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ .env

```bash
# –î–æ–±–∞–≤–∏—Ç—å –≤ .env
echo "USE_GPU=true" >> .env
```

### –ó–∞–ø—É—Å–∫

```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∫—Ä–∏–ø—Ç setup —Å GPU
./scripts/setup_conda.sh
# –í—ã–±—Ä–∞—Ç—å "Yes" –∫–æ–≥–¥–∞ —Å–ø—Ä–æ—Å–∏—Ç –ø—Ä–æ GPU

# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å—ã
./scripts/start_services.sh
```

## –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU —Ä–∞–±–æ—Ç—ã

### Health Check

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á–µ—Ä–µ–∑ API
curl http://localhost:30000/health

# –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
# {
#   "status": "healthy",
#   "gpu": "available (NVIDIA GeForce RTX 3080)",
#   ...
# }
```

### –õ–æ–≥–∏

```bash
# Docker
docker-compose -f docker-compose.gpu.yml logs backend | grep -i cuda
docker-compose -f docker-compose.gpu.yml logs backend | grep -i gpu

# Conda
tail -f logs/backend.log | grep -i cuda

# –î–æ–ª–∂–Ω–æ –±—ã—Ç—å:
# CUDA is available! GPU: NVIDIA GeForce RTX 3080
# GPU acceleration ENABLED
# InsightFace model loaded successfully with providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']
```

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –û–∂–∏–¥–∞–µ–º–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ

| –û–ø–µ—Ä–∞—Ü–∏—è | CPU (Ryzen 9) | GPU (RTX 3080) | –£—Å–∫–æ—Ä–µ–Ω–∏–µ |
|----------|---------------|----------------|-----------|
| Face detection | 0.5s | 0.08s | ~6x |
| Face embedding | 0.5s | 0.05s | ~10x |
| Batch (100 faces) | 50s | 5s | ~10x |
| OCR (Surya) | 5s | 3s | ~1.7x |

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
# –¢–µ—Å—Ç —á–µ—Ä–µ–∑ Python
import time
from backend.app.services.face_recognition import face_recognition_service

# –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
image_path = "test_image.jpg"

# –¢–µ—Å—Ç –±–µ–∑ batch
start = time.time()
faces = face_recognition_service.detect_faces(image_path)
elapsed = time.time() - start
print(f"Single image: {elapsed:.3f}s, detected {len(faces)} faces")

# Batch —Ç–µ—Å—Ç
import glob
images = glob.glob("test_images/*.jpg")[:100]

start = time.time()
for img in images:
    faces = face_recognition_service.detect_faces(img)
elapsed = time.time() - start

print(f"Batch (100 images): {elapsed:.3f}s")
print(f"Average per image: {elapsed/100:.3f}s")
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU

### Real-time –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```bash
# –°–º–æ—Ç—Ä–µ—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
watch -n 1 nvidia-smi

# –ò–ª–∏ –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ
nvidia-smi dmon -s pucvmet
```

### –õ–æ–≥–∏ GPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```bash
# –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –≤ —Ñ–∞–π–ª
nvidia-smi --query-gpu=timestamp,name,utilization.gpu,utilization.memory,memory.used,memory.total \
  --format=csv -l 1 > gpu_usage.log
```

## Troubleshooting

### CUDA not available

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –¥—Ä–∞–π–≤–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
nvidia-smi

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å CUDA –≤ Docker
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ PyTorch –≤–∏–¥–∏—Ç CUDA
python -c "import torch; print(torch.version.cuda)"
```

### Out of Memory (OOM)

–ï—Å–ª–∏ –ø–æ–ª—É—á–∞–µ—Ç–µ –æ—à–∏–±–∫–∏ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –ø–∞–º—è—Ç–∏:

```bash
# –í .env —É–º–µ–Ω—å—à–∏—Ç—å batch size
BATCH_SIZE=16  # –±—ã–ª–æ 32

# –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä detection
FACE_DETECTION_SIZE=480  # –±—ã–ª–æ 640

# –£–º–µ–Ω—å—à–∏—Ç—å concurrency –≤ Celery
# –í docker-compose.gpu.yml:
command: celery -A app.celery_app worker --loglevel=info --concurrency=4  # –±—ã–ª–æ 8
```

### –ú–µ–¥–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å GPU

```bash
# –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU, –∞ –Ω–µ CPU
docker-compose -f docker-compose.gpu.yml logs backend | grep -i provider

# –î–æ–ª–∂–Ω–æ –±—ã—Ç—å CUDAExecutionProvider, –∞ –Ω–µ CPUExecutionProvider
```

### Mixed GPU/CPU

–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á:

```python
# –í .env
USE_GPU=true  # –û–±—â–∏–π —Ñ–ª–∞–≥

# –í –∫–æ–¥–µ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
# backend/app/services/face_recognition.py
# –ò–∑–º–µ–Ω–∏—Ç—å –ª–æ–≥–∏–∫—É _load_model() –ø–æ–¥ —Å–≤–æ–∏ –Ω—É–∂–¥—ã
```

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CPU –≤–µ—Ä—Å–∏—é (–±—ã—Å—Ç—Ä–µ–µ —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è Docker, –º–µ–Ω—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
- GPU –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è production –Ω–∞–≥—Ä—É–∑–∫–∏

### –î–ª—è production
- –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU –≤–µ—Ä—Å–∏—é
- –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É GPU
- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π restart –ø—Ä–∏ OOM

### –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

–î–ª—è RTX 3080 (10GB VRAM):
```env
BATCH_SIZE=32
CELERY_WORKERS=8
FACE_DETECTION_SIZE=640
```

–î–ª—è GTX 1660 (6GB VRAM):
```env
BATCH_SIZE=16
CELERY_WORKERS=4
FACE_DETECTION_SIZE=480
```

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [NVIDIA CUDA Installation Guide](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html)
- [PyTorch CUDA Documentation](https://pytorch.org/docs/stable/cuda.html)
- [ONNX Runtime GPU](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html)
- [InsightFace GPU Setup](https://github.com/deepinsight/insightface/tree/master/python-package)
